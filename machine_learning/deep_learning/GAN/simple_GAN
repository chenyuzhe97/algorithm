# %%
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import torchvision
from torchvision import transforms

# %%
torch.__version__

# %% [markdown]
# 流程图:
# 1.导入库
# 2.准备阶段
#   2.1归一化
#   2.2获取数据集
# 3.网络模型的定义
#   3.1定义生成器
#   3.2定义判别器
# 4.初始化模型
# 5.绘图函数(可选)
# 6.GAN的训练
#   6.1判别器损失的建立
#   6.2生成器损失的建立

# %%
# 对输入数据做归一化(-1,1)
transform = transforms.Compose([
    transforms.ToTensor(),   # 0-1; channel,high,witch,
    transforms.Normalize(0.5,0.5)     # Normalize(均值,方差)
])

# %%
# 获取数据集
train_ds = torchvision.datasets.MNIST('data',
train=True,
transform=transform,
download=True
)

# %%
# 获取训练集
dataloader = torch.utils.data.DataLoader(train_ds,batch_size=64,shuffle=True)

# %% [markdown]
# 定义生成器
# 

# %%
# 输入时长度为 100 的噪声 (正态分布随机数)
# 输出为(1,28,28)的图片
"""
linear 1: 100----256
linear 2: 256----512
linear 3: 512----28*28
reshape: 28*28----(1,28,28)
"""

# %%
class Generator(nn.Module):
    def __init__(self):
        super(Generator,self).__init__()
        self.fc = nn.Linear(100,784)
        self.br = nn.Sequential(
            nn.BatchNorm2d(1),
            nn.LeakyReLU()
        )
        self.downsample1 = nn.Sequential(
            nn.Conv2d(1,12,3,stride=1,padding=1),
            nn.BatchNorm2d(12),
            nn.LeakyReLU(0.1,True)
        )
        self.downsample2 = nn.Sequential(
            nn.Conv2d(12,6,3,stride=1,padding=1),
            nn.BatchNorm2d(6),
            nn.LeakyReLU(0.1,True)
        )
        self.downsample3 = nn.Sequential(
            nn.Conv2d(6,1,3,stride=1,padding=1),
            nn.Tanh()
        )
    def forward(self,x):
        x = self.fc(x)
        x = x.view(x.size(0),1,28,28)
        x = self.br(x)
        x = self.downsample1(x)
        x = self.downsample2(x)
        x = self.downsample3(x)
        return x


# %% [markdown]
# 定义判别器

# %%
# 输入为(1,28,28)的图片 输出为二分类的概率值,输出使用1sigmoid激活
# BCEloss计算交叉熵损失

# nn.LeakyReLU  f(x) :x>0 输出0 , 如果x<0 ,输出 a*x a表示很小的斜率，一般0.1
# 判别器中一般推荐使用 LeakyReLU

# %%
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator,self).__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(1,6,3,padding=2),
            nn.LeakyReLU(0.2,True),
            nn.MaxPool2d(2,stride=2)
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(6,12,3,padding=2),
            nn.LeakyReLU(0.2,True),
            nn.MaxPool2d(2,stride=2)
        )
        self.fc = nn.Sequential(
            nn.Linear(12*8*8,1024),
            nn.LeakyReLU(0.2,True),
            nn.Linear(1024,1),
            nn.Sigmoid()
        )

    def forward(self,x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = x.view(x.size(0),-1)
        x = self.fc(x)
        return x

# %%
device = 'cuda' 

# %%
# 初始化模型
gen = Generator().to(device)
dis = Discriminator().to(device)

# %%
# 初始化优化器
d_optim = torch.optim.Adam(dis.parameters(),lr=0.0001) #优化判别器的模型
g_optim = torch.optim.Adam(gen.parameters(),lr=0.0001) #优化生成器的模型

# %%
# 初始化损失函数
loss_fn = torch.nn.BCELoss()   #计算二元交叉熵

# %% [markdown]
# 绘图函数

# %%
def gen_img_plot(model,test_input,epoch):
    prediction = np.squeeze(model(test_input).detach().cpu().numpy())
    fig = plt.figure(figsize=(4,4))
    for i in range(16):
        plt.subplot(4,4,i+1)
        plt.imshow((prediction[i]+1)/2)     # prediction 是因为生成器最后用tanh() 范围在(-1,1) 要使得变为(0,1)
        plt.axis('off')
    name=str(epoch)+'.jpg'
    plt.savefig(name)
    plt.show()

# %%
# test_input = torch.randn(16,100,device=device)       # 随机噪声的输入

# %% [markdown]
# GAN的训练

# %%
D_loss = [] #存储损失函数
G_loss = [] 

# %%
# 训练循环
for epoch in range(500):
    test_input = torch.randn(16,100,device=device) # 随机噪声输入
    d_epoch_loss = 0  # 计算每一个epoch的平均loss
    g_epoch_loss = 0
    count = len(dataloader)  # 返回样本数
    for step, (img, _) in enumerate(dataloader):
        img = img.to(device)
        size = img.size(0)  # 返回img的第一维度大小，就是batch_size大小64
        random_noise = torch.randn(size, 100, device=device)

        # 1.判别器损失的构建
        d_optim.zero_grad()  # 将上一步的梯度归零

        real_output = dis(img)  # 对判别器输入真实图片，对真实图片的预测结果
        d_real_loss = loss_fn(real_output,  # 希望真实图与全1数组进行比较
                              torch.ones_like(real_output),  # 得到判别器在真实图像上的损失
                              )
        d_real_loss.backward()  # 计算梯度

        gen_img = gen(random_noise)  # 生成器生成图片
        fake_output = dis(gen_img.detach())  # 判别器输入生成图片，对生成图片的预测结果，一定要截断梯度
        d_fake_loss = loss_fn(fake_output,  # 希望生成图与全0数组进行比较
                              torch.zeros_like(fake_output),  # 得到判别器在生成图像上的损失
                              )
        d_fake_loss.backward()  # 计算梯度

        d_loss = d_real_loss + d_fake_loss
        d_optim.step()

        # 2.生成器损失的构建
        g_optim.zero_grad()  # 剃度归零
        fake_output = dis(gen_img)
        g_loss = loss_fn(fake_output,  # 生成器的损失
                         torch.ones_like(fake_output),
                         )
        g_loss.backward()
        g_optim.step()

        with torch.no_grad():  # 总损失
            d_epoch_loss += d_loss
            g_epoch_loss += g_loss
    with torch.no_grad():
        d_epoch_loss /= count
        g_epoch_loss /= count
        D_loss.append(d_epoch_loss)
        G_loss.append(g_epoch_loss)
        print('Epoch:', epoch)
        print('d_epoch_loss:{},g_epoch_loss{}'.format(d_epoch_loss,g_epoch_loss))
        gen_img_plot(gen, test_input,epoch)
